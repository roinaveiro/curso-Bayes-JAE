---
title: "JAEBayeslab 2. Intro MCMC"
author: "David Ríos Insua"
date: " "
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Muestreo de la normal multivariante

Comparamos los métodos vistos en las slides.

Consideramos muestreo de la normal multivariante mediante.
Primero recordamos (para comparar luego)
el muestreo directo. Cambiamos rho a 0 y mu a (0,0) del ejemplo de las slides para simplificar
la discusión.

Muestreo directo.
```{r, eval=FALSE }
library(MASS)
rho<-0
Sigma<-matrix(c(1,rho,rho,1),2,2)
Sigma
mu<-c(0,0)
mu
z1<-mvrnorm(1000,mu,Sigma)
```
Ahora el muestreador de Gibbs según el esquema visto. 
```{r, eval=FALSE }
x_update<-function()
{ x1<-rnorm(1,mu[1]+rho*(x2-mu[2]),sqrt(1-rho*rho))  }
y_update<-function()
{  x2<-rnorm(1,mu[2]+rho*(x1-mu[1]),sqrt(1-rho*rho)) } 
iter<-1500
burnin<-500
set.seed(1)
xg<-matrix(0,2,iter)
x2<-rnorm(1,1,1)
for (i in 1:iter)
{x1<-x_update()
xg[1,i]<-x1
x2<-y_update()
xg[2,i]<-x2}
```
Aquí ponemos el MH con estructura similar. Como distribución generadora de candidatos
empleamos una normal centrada en el estado actual y escalada 1/5 de su tamaño,
esto es $q(\hat {\theta } | \theta )\sim N (\theta , 0.2^2 I)$. Por ser simétrica,
se cancelan en la probabilidad de aceptación los términos correspondientes. 
```{r, eval=FALSE }
iter<-1500
burnin<-500
set.seed(1)
xmh<-matrix(0,2,iter)
xmh[1,1]<-rnorm(1,1,1)
xmh[2,1]<-rnorm(1,1,1)
for(i in 2:iter){
  currentx = xmh[1,i-1]
  currenty=xmh[2,i-1]
  proposedx = rnorm(1,mean=currentx,sd=0.2)
  proposedy = rnorm(1,mean=currenty,sd=0.2)
  A =(dnorm(proposedx,0,1)*dnorm(proposedy,0,1))/(dnorm(currentx,0,1)*dnorm(currenty,0,1)) 
  if(runif(1)<A){
    # aceptamos con probabilidad min (1,A)
    xmh[1,i] = proposedx
    xmh[2,i] = proposedy
  } else {
    # en otro caso rechazamos
    xmh[1,i] = currentx 
    xmh[2,i] = currenty
  }
}
```
Aquí hacemos HMC. Adaptamos primero una función sencilla del gran Radford Neal.

Los argumentos que requiere son

U, función que evalúa menos la log posterior (más una constante)

grad_U, función que evalúa el gradiente de U 

epsilon, paso del leapfrog

L , número de saltos leapfrog

current_q, estado actual 

Los momentos $p$ se muestrean de normales estándar (M es la identidad).
Devuelve el nuevo estado (que será el mismo si se rechaza la 
propuesta realizada después de L saltos leapfrog)

```{r, eval=FALSE }
HMC = function (U, grad_U, epsilon, L, current_q)
{
  q = current_q
  p = rnorm(length(q),0,1)  # normales independientes para generar momentos
  current_p = p
  # Medio paso del momento al principo 
  p = p - epsilon * grad_U(q) / 2
  # L pasos enteros para posición y momento 
  for (i in 1:L)
  {
    # Paso completo para posición 
    q = q + epsilon * p
    # Paso completo para momento, salvo en último paso 
    if (i!=L) p = p - epsilon * grad_U(q)
  }
  # Ultimo medio paso al final para el momento 
  p = p - epsilon * grad_U(q) / 2
   # Evaluamos energias potencial y cinética al principio y... 
  current_U = U(current_q)
  current_K = sum(current_p^2) / 2
  # al final de la trayectoria
  proposed_U = U(q)
  proposed_K = sum(p^2) / 2
  # Decisón de aceptar o rechazar propuesta. exp(...) es probabilidad de aceptar
  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K))
  {
    return (q)  # acepta
  }
  else
  {
    return (current_q)  # rechaza
  }
}
```
Una vez fija esta función se puede aplicar en cualquier problema. 
Para empezar definimos la
log posterior... 
```{r, eval=FALSE }
U=function (param )
{   u = .5 * (param[1]**2 + param[2]**2)
        return(u) 
}
```
Y su gradiente. OJO aquí parece silly la función, pero así es el gradiente en este caso
dado el mu y el y ...
```{r, eval=FALSE}
grad_U=function ( param  )
{     return (param)}
```
Y ahora ya lo aplicamos. Este trozo sería también ya común (esencialmente).
Definimos los parámetros básicos y estructuras de datos inciales.
```{r, eval=FALSE }
iter<-1500
burnin<-500
epsilon=0.1
L=5
set.seed(1)
xhmc<-matrix(0,2,iter)
current_q<-matrix(0,2)
```
Y ahora ya el hmc, inicializamos y lanzamos.
```{r, eval=FALSE}
xhmc[1,1]<-rnorm(1,1,1)
xhmc[2,1]<-rnorm(1,1,1)
for(i in 2:iter){
  current_q[1] = xhmc[1,i-1]
  current_q[2]=xhmc[2,i-1]
  newq= HMC(U, grad_U, epsilon, L, current_q)
    xhmc[1,i] = newq[1] 
    xhmc[2,i] = newq[2]
  }
```

Finalmente representamos.
```{r, eval=FALSE }
par(mfrow=c(2,2))
plot(z1)
plot(xg[1,burnin:iter],xg[2,burnin:iter])
plot(xmh[1,burnin:iter],xmh[2,burnin:iter])
plot(xhmc[1,burnin:iter],xhmc[2,burnin:iter])
# Ojo recordad las acfs!!!!
par(mfrow=c(1,3))
acf(xg[1,burnin:iter])
acf(xmh[1,burnin:iter])
acf(xhmc[1,burnin:iter])
```





